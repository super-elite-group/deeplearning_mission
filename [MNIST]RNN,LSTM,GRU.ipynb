{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkh2qVXxL3Bw"
      },
      "source": [
        "# 1. 데이터 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WWUtmkOxLovz"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as T\n",
        "import torchvision\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "download_root = './MNIST_DATASET'\n",
        "\n",
        "mnist_transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(download_root, transform=mnist_transform, train=True, download=False)\n",
        "test_dataset = torchvision.datasets.MNIST(download_root, transform=mnist_transform, train=False, download=False)\n",
        "\n",
        "total_size = len(train_dataset)\n",
        "train_num, valid_num = int(total_size * 0.8), int(total_size * 0.2)\n",
        "train_dataset,valid_dataset = torch.utils.data.random_split(train_dataset, [train_num, valid_num])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArpCSOs3Wpbf",
        "outputId": "84a5af4d-a01e-4e56-db7a-ec42fdfa679d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "for data, label in train_dataloader:\n",
        "    print(data.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixyFM52zRKhG"
      },
      "source": [
        "# 2.모델 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1w_-DVA5RNmv"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning import LightningModule, Trainer\n",
        "import torch.optim as optim\n",
        "import torchmetrics\n",
        "import torch.nn as nn\n",
        "\n",
        "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "k6AXt6sJL6r7"
      },
      "outputs": [],
      "source": [
        "class RNNClassifire(LightningModule):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes, lr):\n",
        "        super().__init__()\n",
        "\n",
        "        self.learning_rate = lr\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.accuracy = torchmetrics.Accuracy(task='multiclass', num_classes = num_classes)\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.rnn = nn.RNN(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, batch_first = True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        INPUT :\n",
        "            x : [batch_size, sequence_length, input_size] => [batch_size, width, height]\n",
        "        OUTPUT :\n",
        "            out : [batch_size, num_classes]\n",
        "        '''\n",
        "        x = x.squeeze()\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr = self.learning_rate)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 1, gamma = 0.5)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "\n",
        "        loss = self.criterion(y_hat, y)\n",
        "\n",
        "        _, preds = torch.max(y_hat, dim = 1)\n",
        "        acc = self.accuracy(preds, y)\n",
        "\n",
        "        self.log(f\"valid_loss\", loss, on_step = False, on_epoch = True, logger = True)\n",
        "        self.log(f\"valid_acc\", acc, on_step = False, on_epoch = True, logger = True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "\n",
        "        loss = self.criterion(y_hat, y)\n",
        "\n",
        "        _, preds = torch.max(y_hat, dim = 1)\n",
        "        acc = self.accuracy(preds, y)\n",
        "\n",
        "        self.log(f\"valid_loss\", loss, on_step = False, on_epoch = True, logger = True)\n",
        "        self.log(f\"valid_acc\", acc, on_step = False, on_epoch = True, logger = True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "\n",
        "        loss = self.criterion(y_hat, y)\n",
        "\n",
        "        _, preds = torch.max(y_hat, dim = 1)\n",
        "        acc = self.accuracy(preds, y)\n",
        "\n",
        "        self.log(f\"test_loss\", loss, on_step = False, on_epoch = True, logger = True)\n",
        "        self.log(f\"test_acc\", acc, on_step = False, on_epoch = True, logger = True)\n",
        "\n",
        "    def predict_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "\n",
        "        _, preds = torch.max(y_hat, dim = 1)\n",
        "\n",
        "        return preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LSTMClassifier(LightningModule):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes, lr):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.num_classes = num_classes\n",
        "        self.learning_rate = lr\n",
        "        \n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.accuracy = torchmetrics.Accuracy(task='multiclass', num_classes = num_classes)\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, batch_first = True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        INPUT\n",
        "            x : [32, 1, 28, 28]\n",
        "        OUTPUT\n",
        "            out : [32, 10]\n",
        "        '''\n",
        "        x = x.squeeze()\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "\n",
        "        return out\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr = self.learning_rate)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 1, gamma = 0.5)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "\n",
        "        loss = self.criterion(y_hat, y)\n",
        "\n",
        "        _, predict = torch.max(y_hat, dim = 1)\n",
        "        acc = self.accuracy(predict, y)\n",
        "\n",
        "        self.log(f\"train_loss\", loss, on_step = False, on_epoch = True, logger = True)\n",
        "        self.log(f\"train_acc\", acc, on_step = False, on_epoch = True, logger = True)\n",
        "        \n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "\n",
        "        loss = self.criterion(y_hat, y)\n",
        "\n",
        "        _, predict = torch.max(y_hat, dim = 1)\n",
        "        acc = self.accuracy(predict, y)\n",
        "\n",
        "        self.log(f\"valid_loss\", loss, on_step = False, on_epoch = True, logger = True)\n",
        "        self.log(f\"valid_acc\", acc, on_step = False, on_epoch = True, logger = True)\n",
        "        \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "\n",
        "        loss = self.criterion(y_hat, y)\n",
        "\n",
        "        _, predict = torch.max(y_hat, dim = 1)\n",
        "        acc = self.accuracy(predict, y)\n",
        "\n",
        "        self.log(f\"test_loss\", loss, on_step = False, on_epoch = True, logger = True)\n",
        "        self.log(f\"test_acc\", acc, on_step = False, on_epoch = True, logger = True)\n",
        "        \n",
        "\n",
        "    def predict_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "\n",
        "        _, predict = torch.max(y_hat, dim = 1)\n",
        "\n",
        "        return predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u4C0ZsEWiXO"
      },
      "source": [
        "# 3. 모델 실행 및 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-z1vesVdWLnx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "c:\\Users\\tjsgh\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type               | Params | Mode \n",
            "---------------------------------------------------------\n",
            "0 | criterion | CrossEntropyLoss   | 0      | train\n",
            "1 | accuracy  | MulticlassAccuracy | 0      | train\n",
            "2 | rnn       | RNN                | 53.2 K | train\n",
            "3 | fc        | Linear             | 1.3 K  | train\n",
            "---------------------------------------------------------\n",
            "54.5 K    Trainable params\n",
            "0         Non-trainable params\n",
            "54.5 K    Total params\n",
            "0.218     Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f046afa87fe4531957f6424d6a8eab1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\tjsgh\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
            "c:\\Users\\tjsgh\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d045c73488c64cf69406743bbbd907e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1633870b70d64231808a926720edcda4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7211bebd98c442cb9a1cfe760c92c01",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd25f061d67147f5ade41e2edfe05548",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89c64c7957e743e0a1ce8e2fd0e1b391",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fedb21e8c5124a38aad63c91ec8f354c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c0ac31e6f8c49be80a064018520d8de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6923af890a2043c6937ec7fc6fc85d49",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef5c1f19ebbe49be95ac0dd7712f40e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99eeda5ea00641d082560fa998ff5ce7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62f6be8429974c8293bb84a2c4e7e4d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "c:\\Users\\tjsgh\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7b7559759ed4c5fb2c523d648948f4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         valid_acc         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9478999972343445     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">        valid_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.17794333398342133    </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m        valid_acc        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9478999972343445    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m       valid_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.17794333398342133   \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[{'valid_loss': 0.17794333398342133, 'valid_acc': 0.9478999972343445}]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = RNNClassifire(input_size = 28, hidden_size = 128, num_layers = 2, num_classes = 10, lr = 0.01)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor = 'valid_loss', mode = 'min')\n",
        "lr_mointor = LearningRateMonitor(logging_interval = 'epoch')\n",
        "\n",
        "wandb_logger = WandbLogger(project = 'MNIST_RNN')\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs = 10,\n",
        "    accelerator = 'auto',\n",
        "    callbacks = [early_stopping, lr_mointor],\n",
        "    logger = wandb_logger\n",
        ")\n",
        "\n",
        "trainer.fit(\n",
        "    model,\n",
        "    train_dataloader,\n",
        "    valid_dataloader\n",
        ")\n",
        "\n",
        "trainer.test(model, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "c:\\Users\\tjsgh\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type               | Params | Mode \n",
            "---------------------------------------------------------\n",
            "0 | criterion | CrossEntropyLoss   | 0      | train\n",
            "1 | accuracy  | MulticlassAccuracy | 0      | train\n",
            "2 | lstm      | LSTM               | 212 K  | train\n",
            "3 | fc        | Linear             | 1.3 K  | train\n",
            "---------------------------------------------------------\n",
            "214 K     Trainable params\n",
            "0         Non-trainable params\n",
            "214 K     Total params\n",
            "0.857     Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8995cf1e33c4b76bce9940d6cacabb6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\tjsgh\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
            "c:\\Users\\tjsgh\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "247b8cd1f8e64b8a97af2a3b0b1e1c05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\tjsgh\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9fdf9d15984c4caebd9b5ac8dd736cc9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c073d809e6164b1d933fa1d103ab9f7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4a640305c8c4da89086b2b39163e4f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc400d07772d487dbcd6d732601bddb6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "c:\\Users\\tjsgh\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c559fbe164441daa7213c0b9b2b0b55",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         train_acc         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.09799999743700027    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">        train_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.3070943355560303     </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m        train_acc        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.09799999743700027   \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m       train_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.3070943355560303    \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[{'train_loss': 2.3070943355560303, 'train_acc': 0.09799999743700027}]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = LSTMClassifier(input_size = 28, hidden_size = 128, num_layers = 2, num_classes = 10, lr = 0.01)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor = 'valid_loss', mode = 'min')\n",
        "lr_mointor = LearningRateMonitor(logging_interval = 'epoch')\n",
        "\n",
        "wandb_logger = WandbLogger(project = 'MNIST_LSTM')\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs = 10,\n",
        "    accelerator = 'auto',\n",
        "    callbacks = [early_stopping, lr_mointor],\n",
        "    logger = wandb_logger\n",
        ")\n",
        "\n",
        "trainer.fit(\n",
        "    model,\n",
        "    train_dataloader,\n",
        "    valid_dataloader\n",
        ")\n",
        "\n",
        "trainer.test(model, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
